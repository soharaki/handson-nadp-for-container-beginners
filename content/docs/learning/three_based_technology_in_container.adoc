---
title: コンテナはなぜ動くのか？(後編)
weight: 31
---

++++
<h1>コンテナはなぜ動くのか？(後編)</h1>
++++

改めて、コンテナの定義をおさらいします。

[quote,コンテナ技術入門 - 仮想化との違いを知り、要素技術を触って学ぼう, https://eh-career.com/engineerhub/entry/2019/02/05/103000]
____
コンテナはコンテナランタイムによって作成された、ホストOSのリソースを**隔離・制限**したプロセスです。
実行するアプリケーションやライブラリを個別に用意し、アプリケーション実行時にプロセスの属性を変更してホストOSのリソースを隔離・制限することで、
独立した実行環境を構築します。
____

それでは、この**隔離・制限**を行うためにどういった技術が使用されているのでしょうか？ +
実はそれにはUnix、Linuxが長年の開発で整えてきた3つの技術要素が関係しています。

## コンテナを支える3つの技術要素

コンテナを支える3つの技術要素とは次の三つを指します。

1. chroot
2. namespace
3. cgroup

### chroot

chrootとは「change root」の略で(諸説あります)、新しいプロセスのルートディレクトリを設定できるLinuxコマンドです。

{{< figure src="/images/chroot.jpg" class="center" height="320" caption="chrootの構造" >}}

ルートディレクトリとは、ざっくり言えば**これ以上は上に行くことができないディレクトリ**のことです。
したがってchrootとはアプリケーションが指定されたディレクトリ以外にアクセスできないように**制限**をかける仕組みだと言えます。

この仕組みにより、コンテナはファイルシステムにアクセスする際に、安全なストレージ領域を確保するとともに、
他のコンテナが干渉しないようにします。

コンテナという言葉が普及する前は、この親ディレクトリの外側に出られない仕組みが監獄に似ていることから +
仮想化と言えば「chroot jail」(監獄)という名前が一般的でした。

[NOTE]
====
chrootは1979年にもともと「Version 7」と呼ばれるUNIXの開発の過程で生まれました。
https://www.itmedia.co.jp/enterprise/articles/1506/10/news001.html
====

### namespace

chrootだけでは当然まだまだ機能は足りません。例えば、プロセス同士の干渉問題があります。

歴史的に、Linux カーネルは単一のプロセスツリーを維持してきました。ツリーには、現在実行中のすべてのプロセスが親子階層で参照されています。
プロセスは十分な権限を持ち、特定の条件を満たしていれば、トレーサーをつけることで他のプロセスを検査したり、そのプロセスをkillすることもできます。

Linuxの**PID namespace**の導入により、複数の「入れ子になった」プロセスツリーを持つことが可能になりました。各プロセスツリーは完全に分離されたプロセスのセットを持つことができます。
これにより、あるプロセスツリーに属するプロセスは、他の兄弟プロセスツリーや親プロセスツリーにあるプロセスを検査したり、killすることはできません。

Linux を搭載したコンピュータが起動するたびに、プロセス識別子 (PID) 1 の 1 つのプロセスで起動します。このプロセスはプロセスツリーのルートであり、
適切なメンテナンス作業を実行したり、適切なデーモン/サービスを起動したりすることで、システムの残りの部分を開始します。
他のすべてのプロセスは、ツリー上でこのプロセスの下から開始します。PID 名前空間では、PID 1 プロセスを持つ新しいツリーをスピンオフすることができます。
これを行うプロセスは元のツリーの親名前空間に残りますが、子プロセスは自分のプロセスツリーのルートになります。

PID 名前空間が分離されているため、子名前空間のプロセスは親プロセスの存在を知ることができません。
しかし、親ネームスペース内のプロセスは、親ネームスペース内の他のプロセスであるかのように、子ネームスペース内のプロセスを完全に把握することができます。

{{<
figure src="/images/namespace.png"
>}}

現在、LinuxカーネルにはPID namespaceを含めてネームスペースは8種類存在します。

* network namespace
* mount namespace
* user namespace
* IPC namespace
* cgroup namespace
* UTS(UNIX Time-Sharing) namespace
* Time namespace

network namespaceはDockerやKubernetes上の仮想ネットワークを実現ために必要不可欠な技術となります。

[NOTE]
====
Dockerコンテナは、開発当初PID namespaceで作成した仮想プロセスにPID "1" を振ってました。

LinuxではPID "1" はルートプロセスを意味して、OSが起動したときに一番最初に起動する重要なプロセスです。
したがって、通常のプロセスとは異なる特性があります。

例えば、PID1以外のプロセスからプロセス終了のためのシステムコールSIGTERMをPID1に送信しても無視します。勝手に終了しては困るからです。
この特性はPID namespaceで子プロセス化してもそのまま引き継がれます。

したがって、コンテナを終了するときにうまくシャットダウンできず、
コンテナ化したアプリケーションによっては終了できないままゾンビプロセス化するバグがありました。
その代表例がNode.jsです。

もちろん、現在では通常使用では上記問題は起こりません。
ですが、プロセスをどうやって終了するか？(graceful shutdown)は
operatorの設計を考慮するための基礎知識として欠かせません。

参考: https://cloud.google.com/solutions/best-practices-for-building-containers[Googleが出しているコンテナに関するベストプラクティス]
参考: https://nodesource.com/blog/8-protips-to-start-killing-it-when-dockerizing-node-js
参考: https://text.superbrothers.dev/200328-how-to-avoid-pid-1-problem-in-kubernetes/
====

### cgroup

chrootやnamespaceで隔離された環境で安全にプロセスが動くようになりましたが、まだまだサーバー上で動かすための課題は残っています。

ホストのすべてのプロセスはCPUやメモリを共有します。そこでとあるプロセスが暴走したとしましょう。
サーバー上のコンピューティング能力には限りがあるので、プロセスが多くのCPUやメモリを消費してしまうと、
新しくアプリケーションを実行できなくなったり動作が遅く不安定になったり、最悪の場合、他のプロセスが勝手に終了されてしまいます。

そんな課題に答えるためにGoogleが開発したのがcgrpupになります。

cgroupはグループ化したプロセスに対してカーネルリソースやハードウェアリソースを制限することができる機能です。
cgroupではプロセスが利用できるリソースを制限し、こうしたリスクを抑制できます。cgroupの機能は具体的に表現すると以下のようになります。

* CPU時間の制限、割り当てCPUの指定
* メモリ使用量の制限、OOM killerの有効化/無効化
* プロセス数の確認と制限
* デバイスのアクセス制御
* ネットワーク優先度設定
* タスクの一時停止/再開
* CPU使用量、メモリ使用量のレポート
* ネットワークパケットをタグ付け(tc で利用)

### 結局、コンテナとは...?

コンテナの要素技術として、 `chroot`、 `namespace`、 `cgroup`　について説明しました。

実はこれらは全てコマンド化されていて個別に実行することでコンテナ環境を作ることができます。
レディメイドなコンテナ環境を作りたい場合、例えば
https://eh-career.com/engineerhub/entry/2019/02/05/103000[コンテナ技術入門 - 仮想化との違いを知り、要素技術を触って学ぼう]という記事が参考になるでしょう。

ただ、このコマンド群を個別に時間をかけて実行してコンテナ環境を構築していたら非常に時間がかかります。
そこでこれのコマンドをwrapする技術が次々と生まれました。

* LXC/LXD
  - IBMが開発
  - cgroupsとnamespaceを使ったコンテナツールの走り
  - 初期のDockerはLXCをwrapしてコンテナを作ってました
* systemd-nspawn
  - chrootの後継
  - Linuxシステム環境を再現したコンテナ環境を作ることができる
  - systemd(Linuxの設定をシンプルにするためのinitシステム)の機能の一つとして
    搭載された軽量コンテナランタイム
* Docker
  - 時系列的には、上記の中では一番最後に開発着手
  - 後で詳細を説明します

この中でダントツに知名度があるのはDockerでしょう。
Dockerはコンテナランタイム環境としては後発組ですが、一番有名なツールです。

どうしてDockerは開発者から圧倒的人気と知名度を得たのでしょうか？
それは、性能でも資本力による広告でもありません。

Docker社によって整備されたコンテナ環境の管理機能の使い勝手が
既存のコンテナ技術と比較して非常に良かったからです。要は利便性です。

Docker社が整備したエコシステムがどのようなものだったかは次の章で説明します。

## ハイパーバイザー型の仮想環境との比較

最後にハイパーバイザー型の仮想環境との違いについて触れておきましょう。
Dockerの説明にあたり、よく下記のようにハイパーバイザー型の仮想環境との比較図が出てくると思います。

{{< figure src="/images/hypervisor_container.gif" class="center" >}}

伝統的にOSは監督者(Supervisor)と呼ばれていました。
Hypervisor(ハイパーバイザー)は、そのOSをさらに管理する監督者(superの上だからhyper)という意味から来ました。(諸説あります)

OSを管理するとはどういうことでしょうか?図で説明するとこんな感じです。

{{< figure src="/images/Hyperviseur.png" class="center" >}}

ハイパーバイザー型にはType1とType2の二種類があるのですが、
共通して言えるのは、複数ある仮想環境のOSを管理するためのプログラムだということです。

Linux環境の一部を切り出してそのまま仮想環境化したコンテナ技術とは全く違う技術だということが分かります。

{{< figure src="/images/diagram_modify.png" class="center" >}}

## コンテナのメリット、デメリット

### メリット

* 高密度化が可能
  - 起動している OS (カーネル) は一つ
* オーバーヘッドが小さい
  - ハードウェアの仮想化が不要
* 起動が早い
  - 仮想マシンの起動ではなく，ホスト OS から見たら単にプロセスが起動しているだけなので，普通のプログラムが起動する
のとほとんど変わらない
* 必ずしもOSのようなシステム全体を動かす必要はない (アプリケーションコンテナ)
  - 例えば、バイナリが直接動くコンテナを作ることができます
[source,docker]
----
FROM scratch
ADD bin/helloworld /helloworld
CMD ["/helloworld"]
----
* コンテナにメモリを固定的に割り当てる必要はありません
  - cgroupの力を借りることで動的な制御が可能です

### デメリット

* 異なるOSのシステム/プログラムは動かせない
  ** コンテナも結局OSから見たら一つのプロセス(プログラム)なので、
    Intelのサーバー環境でARMのプログラムは動きません
  ** 後の章で説明しますが、コンテナを補完するためのコンテナレジストリ(例えばDockerhub)には
    どのCPUに対応しているか、大体のサービスで明記しています。
    {{< figure src="/images/dockerhub.jpg" class="center" >}}
  ** Hypervisoer型の仮想環境はOSそのものをプログラムで再現するため、
    コンテナ型のような制約はありません
* カーネルに関する操作をするためには特権権限(Privileged Mode)が必要
  ** 特権権限がついていると、なんでもできてしまうため、コンテナのメリットであるセキュリティ確保ができません
  ** この制約に関してはCapabilityと呼ばれるシステムコールを部分的に許可する仕組みの実装や
      seccomp notifyと呼ばれる動的にシステムコールの制限を解除する仕組み(こちらは対応中)によって
      徐々にデメリットがなくなりつつあります
* GPUに関する知見は正直まだまだ少ない
  ** https://medium.com/nttlabs/docker-1903-5155754ff8ac[DockerコマンドがGPU(NVIDIA)正式に対応したのは2019年]
    *** それまではコンテナでGPUを利用した機械学習をしたい場合、NVIDIAが開発した専用ツールを使用する必要がありました
    *** 2013年にDockerがデビューしたことを考えると結構時間がかかりました
  ** GPU対応が遅れた理由=>コンテナの技術要素はGPUとは何の関係もないから
    *** コンテナからGPUを利用するにはNVIDIAなどGPUベンダーが提供している専用のデバイスドライバを利用する必要がある
    *** デバイスドライバを利用するためには当然システムコールを発行する必要があるので、前述で説明したCapabilityの設定が必要
　** https://medium.com/nvidiajapan/nvidia-docker-%E3%81%A3%E3%81%A6%E4%BB%8A%E3%81%A9%E3%81%86%E3%81%AA%E3%81%A3%E3%81%A6%E3%82%8B%E3%81%AE-20-09-%E7%89%88-558fae883f44[現在の対応状況はNVIDIAの中の人が記事を書いている]
    *** NVIDIA Container Toolkitというフレームワークを利用する必要があります
    *** https://docs.rapidminer.com/9.7/deployment/docker-compose/deep-learning.html[RapidMinerではDeepLearning環境で使用している模様]
    *** {{< figure src="/images/nvidia.jpg" class="center" >}}

## 振り返り

仮想化技術の永遠のテーマを思い出してください。

1. マシンリソースの分配
  - 複数のユーザー、プログラムで、マシンリソースをいかに効率よく使用するか(共用するか)
2. セキュリティの担保
  - 1の条件下で、いかにセキュリティを担保するか

コンテナと呼ばれる仮想化技術もこのテーマを解決するための技術的発展形の一つにすぎません。
ですが、個々の技術要素の詳細を抑えることで、仮にコンテナに関して技術的トラブルが発生しても
トラブルシューティングしやすくなったかと存じます。

次の章ではいよいよDockerコンテナについて説明します。

## 参考文献
* https://btholt.github.io/complete-intro-to-containers/chroot